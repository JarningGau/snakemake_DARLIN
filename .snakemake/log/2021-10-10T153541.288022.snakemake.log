Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 2
Rules claiming more threads will be scaled down.
Job stats:
job               count    min threads    max threads
--------------  -------  -------------  -------------
download_fastq        1              1              1
total                 1              1              1

Select jobs to execute...

[Sun Oct 10 15:35:42 2021]
rule download_fastq:
    jobid: 0
    resources: tmpdir=/tmp

[Sun Oct 10 15:35:42 2021]
Error in rule download_fastq:
    jobid: 0
    shell:
        mkdir -p /n/data1/bch/hemonc/camargo/li/DATA/20211010_3_pulses_cCARLIN_Tigrebs download project 3_pulses_LL605_LL607 --id 295673483 -o /n/data1/bch/hemonc/camargo/li/DATA/20211010_3_pulses_cCARLIN_Tigremkdir -p /n/data1/bch/hemonc/camargo/li/DATA/20211010_3_pulses_cCARLIN_Tigre/raw_fastqmv /n/data1/bch/hemonc/camargo/li/DATA/20211010_3_pulses_cCARLIN_Tigre/*/*.fastq.gz /n/data1/bch/hemonc/camargo/li/DATA/20211010_3_pulses_cCARLIN_Tigre/raw_fastqrm -r /n/data1/bch/hemonc/camargo/li/DATA/20211010_3_pulses_cCARLIN_Tigre/*_ds.*
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /n/data1/bch/hemonc/camargo/li/my_scripts/snakeTemplate/.snakemake/log/2021-10-10T153541.288022.snakemake.log
